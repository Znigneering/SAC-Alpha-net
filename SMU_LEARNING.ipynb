{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.signal\n",
    "\n",
    "import core\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "\n",
    "def combined_shape(length, shape=None):\n",
    "    if shape is None:\n",
    "        return (length,)\n",
    "    return (length, shape) if np.isscalar(shape) else (length, *shape)\n",
    "\n",
    "def mlp(sizes, activation, output_activation=nn.Identity):\n",
    "    layers = []\n",
    "    for j in range(len(sizes)-1):\n",
    "        act = activation if j < len(sizes)-2 else output_activation\n",
    "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def count_vars(module):\n",
    "    return sum([np.prod(p.shape) for p in module.parameters()])\n",
    "\n",
    "\n",
    "LOG_STD_MAX = 2\n",
    "LOG_STD_MIN = -20\n",
    "\n",
    "class SquashedGaussianMLPActor(nn.Module):\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation, act_limit):\n",
    "        super().__init__()\n",
    "        self.net = mlp([obs_dim] + list(hidden_sizes), activation, activation)\n",
    "        self.mu_layer = nn.Linear(hidden_sizes[-1], act_dim)\n",
    "        self.log_std_layer = nn.Linear(hidden_sizes[-1], act_dim)\n",
    "        self.act_limit = act_limit\n",
    "\n",
    "    def forward(self, obs, deterministic=False, with_logprob=True):\n",
    "        net_out = self.net(obs)\n",
    "        mu = self.mu_layer(net_out)\n",
    "        log_std = self.log_std_layer(net_out)\n",
    "        log_std = torch.clamp(log_std, LOG_STD_MIN, LOG_STD_MAX)\n",
    "        std = torch.exp(log_std)\n",
    "\n",
    "        # Pre-squash distribution and sample\n",
    "        pi_distribution = Normal(mu, std)\n",
    "        if deterministic:\n",
    "            pi_action = mu\n",
    "        else:\n",
    "            pi_action = pi_distribution.rsample()\n",
    "\n",
    "        if with_logprob:\n",
    "            logp_pi = pi_distribution.log_prob(pi_action).sum(axis=-1)\n",
    "            logp_pi -= (2*(np.log(2) - pi_action - F.softplus(-2*pi_action))).sum(axis=1)\n",
    "        else:\n",
    "            logp_pi = None\n",
    "\n",
    "        pi_action = torch.tanh(pi_action)\n",
    "        pi_action = self.act_limit * pi_action\n",
    "\n",
    "        return pi_action, logp_pi\n",
    "    \n",
    "class MLPMutiQ(nn.Module):\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation):\n",
    "        super().__init__()\n",
    "        self.q1 = mlp([obs_dim + act_dim] + list(hidden_sizes) + [1], activation)\n",
    "        self.q2 = mlp([obs_dim + act_dim] + list(hidden_sizes) + [1], activation)\n",
    "        # self.q3 = mlp([obs_dim + act_dim] + list(hidden_sizes) + [1], activation)\n",
    "\n",
    "    def forward(self, obs, act):\n",
    "        input = torch.cat([obs, act], dim=-1)\n",
    "        q1 = self.q1(input)\n",
    "        q2 = self.q2(input)\n",
    "        # q3 = self.q3(input)\n",
    "\n",
    "        output = torch.stack([torch.squeeze(q1, -1),torch.squeeze(q2, -1)])\n",
    "        # output = torch.stack([torch.squeeze(q1, -1),torch.squeeze(q2, -1),torch.squeeze(q2, -1)])\n",
    "        return output # Critical to ensure q has right shape.\n",
    "    \n",
    "class MLPActorCriticMutiQ(nn.Module):\n",
    "\n",
    "    def __init__(self, observation_space, action_space, num_out=10, hidden_sizes=(256,256),\n",
    "                 activation=nn.ReLU):\n",
    "        super().__init__()\n",
    "\n",
    "        obs_dim = observation_space.shape[0]\n",
    "        act_dim = action_space.shape[0]\n",
    "        act_limit = action_space.high[0]\n",
    "\n",
    "        # build policy and value functions\n",
    "        self.pi = SquashedGaussianMLPActor(obs_dim, act_dim, hidden_sizes, activation, act_limit)\n",
    "        self.value = MLPMutiQ(obs_dim,act_dim,hidden_sizes=hidden_sizes,activation=activation)\n",
    "        self.alpha_net = mlp([obs_dim] + [obs_dim,obs_dim] + [1], activation)\n",
    "\n",
    "    def act(self, obs, deterministic=False):\n",
    "        with torch.no_grad():\n",
    "            a, _, = self.pi(obs, deterministic, False)\n",
    "            return a.numpy()\n",
    "        \n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    A simple FIFO experience replay buffer for SAC agents.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, size):\n",
    "        self.obs_buf = np.zeros(core.combined_shape(size, obs_dim), dtype=np.float32)\n",
    "        self.obs2_buf = np.zeros(core.combined_shape(size, obs_dim), dtype=np.float32)\n",
    "        self.act_buf = np.zeros(core.combined_shape(size, act_dim), dtype=np.float32)\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.done_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ptr, self.size, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, obs, act, rew, next_obs, done):\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.obs2_buf[self.ptr] = next_obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.done_buf[self.ptr] = done\n",
    "        self.ptr = (self.ptr+1) % self.max_size\n",
    "        self.size = min(self.size+1, self.max_size)\n",
    "\n",
    "    def sample_batch(self, batch_size=32):\n",
    "        idxs = np.random.randint(0, self.size, size=batch_size)\n",
    "        batch = dict(obs=self.obs_buf[idxs],\n",
    "                     obs2=self.obs2_buf[idxs],\n",
    "                     act=self.act_buf[idxs],\n",
    "                     rew=self.rew_buf[idxs],\n",
    "                     done=self.done_buf[idxs])\n",
    "        return {k: torch.as_tensor(v, dtype=torch.float32) for k,v in batch.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import gymnasium as gym\n",
    "import time\n",
    "import core\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "from IPython import display\n",
    "# plt.ion()\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "fig.subplots_adjust(right=0.85)\n",
    "ax2 = ax1.twinx()\n",
    "def plot_durations(steps_per_epoch,history_rewards,alpha_history,env_name,save_fig=False):\n",
    "    # display.clear_output(wait=True)\n",
    "    ax1.clear()\n",
    "    ax2.clear()\n",
    "    t = [i*steps_per_epoch for i in range(len(history_rewards))]\n",
    "\n",
    "    color = 'tab:red'\n",
    "    ax1.set_title('Training...')\n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.set_ylabel('Duration')\n",
    "    ax1.plot(t,history_rewards,color=color)\n",
    "    ax1.hlines(max(history_rewards),xmin=0,xmax=len(history_rewards)*steps_per_epoch,color='orange',linestyles='dashdot')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    # ax2 = ax1.twinx()  # instantiate a second Axes that shares the same x-axis\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('alpha', color=color)  # we already handled the x-label with ax1\n",
    "    ax2.yaxis.set_label_position('right')\n",
    "    ax2.plot(t,alpha_history, color=color)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    # plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    # fig.tight_layout()\n",
    "    # plt.show()\n",
    "    # display.display(fig)\n",
    "    fig.savefig(f\"./mutiQ_{env_name}\")\n",
    "\n",
    "\n",
    "def sac(env,test_env,env_name, actor_critic=MLPActorCriticMutiQ, ac_kwargs=dict(), seed=0, \n",
    "        steps_per_epoch=4000, epochs=100, replay_size=int(1e6), gamma=0.99, \n",
    "        polyak=0.995, lr=1e-3, alpha_base =0.2, final_entropy=-1,alpha_lr=1e-4, batch_size=100, start_steps=10000, \n",
    "        update_after=1000, update_every=1, num_test_episodes=10, max_ep_len=1000, lta = 1,\n",
    "        logger_kwargs=dict(), save_freq=1):\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    history_rewards = []\n",
    "    history_avg_alpha = []\n",
    "\n",
    "    obs_dim = env.observation_space.shape\n",
    "    act_dim = env.action_space.shape[0]\n",
    "\n",
    "    # Action limit for clamping: critically, assumes all dimensions share the same bound!\n",
    "    act_limit = env.action_space.high[0]\n",
    "\n",
    "    # Create actor-critic module and target networks\n",
    "    ac = actor_critic(env.observation_space, env.action_space, **ac_kwargs)\n",
    "    with torch.no_grad():\n",
    "        for p in ac.alpha_net.parameters():\n",
    "            p[:] = 0\n",
    "    ac_targ = deepcopy(ac)\n",
    "    \n",
    "\n",
    "    # Freeze target networks with respect to optimizers (only update via polyak averaging)\n",
    "    for p in ac_targ.parameters():\n",
    "        p.requires_grad = False\n",
    "        \n",
    "    # Experience buffer\n",
    "    replay_buffer = ReplayBuffer(obs_dim=obs_dim, act_dim=act_dim, size=replay_size)\n",
    "\n",
    "    # Set up optimizers for policy and q-function\n",
    "    pi_optimizer = Adam(ac.pi.parameters(), lr=lr)\n",
    "    q_optimizer = Adam(ac.value.parameters(), lr=lr)\n",
    "    alpha_optimizer = Adam(ac.alpha_net.parameters(), lr=alpha_lr)\n",
    "\n",
    "    # unit_tensor = torch.tensor(np.zeros(obs_dim),dtype=torch.float32)\n",
    "\n",
    "\n",
    "    def update(data):\n",
    "\n",
    "        # First run one gradient descent step for Q1 and Q2\n",
    "        o, a, r, o2, d = data['obs'], data['act'], data['rew'], data['obs2'], data['done']\n",
    "        r = r * 5\n",
    "        \n",
    "        log_alpha = ac.alpha_net(prev_o + lta * (o - prev_o))\n",
    "        # alpha = log_alpha\n",
    "        alpha = torch.exp(log_alpha) + alpha_base\n",
    "        # alpha = torch.sigmoid(log_alpha) + alpha_base\n",
    "        q_optimizer.zero_grad()\n",
    "        qs = ac.value(o,a)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            a2, logp_a2 = ac.pi(o2)\n",
    "\n",
    "            q_targs = ac_targ.value(o2,a2)\n",
    "            q_targs_min = q_targs.min(dim=0)[0]\n",
    "\n",
    "            backup = r + gamma * (1 - d) * (q_targs_min - alpha.detach() * logp_a2)\n",
    "        loss_q = ((qs[0] - backup)**2).mean()\n",
    "        for q in qs[1:]:\n",
    "            loss_q += ((q - backup)**2).mean()\n",
    "        loss_q.backward()\n",
    "        q_optimizer.step()\n",
    "        \n",
    "        \n",
    "        for p in ac.value.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        pi_optimizer.zero_grad()\n",
    "        pi, logp_pi = ac.pi(o)\n",
    "\n",
    "        qs_pi = ac.value(o,pi)\n",
    "        q_pi_min = qs_pi.min(dim=0)[0]\n",
    "\n",
    "        loss_pi = (alpha.detach() * logp_pi - q_pi_min).mean()\n",
    "        loss_pi.backward()\n",
    "        pi_optimizer.step()\n",
    "\n",
    "        for p in ac.value.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "        # alpha_loss = -(alpha * (logp_pi.detach() + final_entropy)).mean()\n",
    "        alpha_loss = -(log_alpha * (logp_pi.detach() + final_entropy)).mean()\n",
    "        alpha_optimizer.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        alpha_optimizer.step()\n",
    "        log_alpha = log_alpha + alpha_loss + 1 * ac.alpha_net(o2 - o)\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for p, p_targ in zip(ac.parameters(), ac_targ.parameters()):\n",
    "                p_targ.data.mul_(polyak)\n",
    "                p_targ.data.add_((1 - polyak) * p.data)\n",
    "\n",
    "        return alpha.detach()\n",
    "\n",
    "    def get_action(o, deterministic=False):\n",
    "        return ac.act(torch.as_tensor(o, dtype=torch.float32), \n",
    "                      deterministic)\n",
    "\n",
    "    def test_agent():\n",
    "        ep_rets = []\n",
    "        for j in range(num_test_episodes):\n",
    "            (o,_), d, ep_ret, ep_len = test_env.reset(), False, 0, 0\n",
    "            while not(d or (ep_len == max_ep_len)):\n",
    "                # Take deterministic actions at test time \n",
    "                o, r, termin, tunc, _ = test_env.step(get_action(o, True))\n",
    "                d = termin or tunc\n",
    "                ep_ret += r\n",
    "                ep_len += 1\n",
    "            ep_rets.append(ep_ret)\n",
    "        return np.average(ep_rets)\n",
    "\n",
    "    # Prepare for interaction with environment\n",
    "    total_steps = steps_per_epoch * epochs\n",
    "    start_time = time.time()\n",
    "    (o,_), ep_ret, ep_len = env.reset(), 0, 0\n",
    "\n",
    "    # Main loop: collect experience in env and update/log each epoch\n",
    "    pbar = tqdm(range(total_steps))\n",
    "    for t in pbar:\n",
    "        if t > start_steps:\n",
    "            a = get_action(o)\n",
    "        else:\n",
    "            a = env.action_space.sample()\n",
    "\n",
    "        # Step the env\n",
    "        o2, r, termin, tunc, _ = env.step(a)\n",
    "        d = termin or tunc\n",
    "        ep_ret += r\n",
    "        ep_len += 1\n",
    "\n",
    "        d = False if ep_len==max_ep_len else d\n",
    "\n",
    "        replay_buffer.store(o, a, r, o2, d)\n",
    "        o = o2\n",
    "\n",
    "        if d or (ep_len == max_ep_len):\n",
    "            (o,_), ep_ret, ep_len = env.reset(), 0, 0\n",
    "\n",
    "        init = True\n",
    "        # Update handling\n",
    "        if t >= update_after and t % update_every == 0:\n",
    "            for j in range(update_every):\n",
    "                batch = replay_buffer.sample_batch(batch_size)\n",
    "                if init:\n",
    "                    prev_o = batch['obs']\n",
    "                    init = False\n",
    "                alpha = update(data=batch)\n",
    "                prev_o = batch['obs']\n",
    "\n",
    "        # End of epoch handling\n",
    "        if (t+1) % steps_per_epoch == 0:\n",
    "            epoch = (t+1) // steps_per_epoch\n",
    "\n",
    "            avg_reward = test_agent()\n",
    "            history_rewards.append(avg_reward)\n",
    "            with torch.no_grad():\n",
    "                history_avg_alpha.append(alpha.mean().numpy())\n",
    "            \n",
    "            plot_durations(steps_per_epoch,history_rewards,history_avg_alpha,env_name,False)\n",
    "            pd.DataFrame(np.transpose([history_rewards,history_avg_alpha]),columns=['reward','alpha']).to_csv(f\"./mutiQ_{env_name}.csv\")\n",
    "\n",
    "            pbar.set_description_str(f\"avg_reward: {avg_reward:.2f}\")\n",
    "\n",
    "    return ac,ac_targ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg_reward: 987.18:  28%|██▊       | 842461/3000000 [1:57:31<7:29:51, 79.93it/s]  "
     ]
    }
   ],
   "source": [
    "torch.set_num_threads(torch.get_num_threads())\n",
    "lta = 10\n",
    "\n",
    "for i in range(0,5):\n",
    "    env_name = 'Ant-v4'\n",
    "    sac(gym.make(env_name),gym.make(env_name),env_name=env_name + f'-lta = {lta}-' + str(i)+'.png', actor_critic=MLPActorCriticMutiQ,\n",
    "    ac_kwargs=dict(num_out=10,hidden_sizes=[256]*2), num_test_episodes=1, steps_per_epoch=4000, start_steps=10000, update_after=1000,\n",
    "    alpha_base=0,final_entropy=-8,alpha_lr=1e-3,lr=1e-3,batch_size=100,\n",
    "    gamma=0.99, seed=np.random.randint(0,1000), epochs=750,lta = 0,\n",
    "    logger_kwargs={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RLB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
